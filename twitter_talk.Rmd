---
title: "Twitter Talk Analysis"
author: "Mariesse van Sluisveld"
---



```{r results=TRUE, message=FALSE, warning=FALSE, echo=FALSE, fig.align="center", fig.width=8, fig.height=5, fig.cap="_**Figure**: number of tweets talking about European business associations"}


################################################################################################
################################################################################################

# Clear memory
rm(list=ls()) # clear memory

# Working directory
#setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

## Required packages
### If not yet availabe, use install.packages("")

# Soc Media API readers
library(rtweet)

# plotting and pipes!
library(ggplot2) 
#library(patchwork)
library(dplyr)
library(tidyr) 
library(data.table)
library(paletteer) 
library(ggrepel)
library(igraph)
library(ggraph)

# text mining library
library(tidytext)
library(tm)   

# coupled words analysis
# library(widyr)

theme_set(
  theme_bw()
            )
```


```{r results=TRUE, message=FALSE, warning=FALSE, echo=FALSE}


########################################################################################
## API set-up (Fill in yourself)
########################################################################################

token <- create_token(
  app = "",
  consumer_key = "XsUTxqZNWovUgkNsFctSasLb7", 
  consumer_secret = "XKg0X52FKVEPMBh4lvMIJZaD0qVuj56vaaPLhsWsJ03xLiZnpD",
  access_token = '3092191180-6hcwjXBiyE7SVoHmCUVHOrr5kdi0pgYxIKq0YM6',
  access_secret = 'N6Fk8IZFyE0F4RTWRFoIjFieMNDUS8M162imYoYipgS50')


########################################################################################
## Your analysis input
########################################################################################

Date <-Sys.Date() 

TopicSearch <- c("CEFIC", "CEPI_Paper", "CEMBUREAU", "EUROFER_eu")
GreenTweet <- c("EII", "EULTS", "ClimateNeutralEU")


stop_words_nl=tibble(word=stopwords("dutch"))
stop_words_du=tibble(word=stopwords("german"))


########################################################################################

##########################################################################
### Pull recent tweets (7 days)
##########################################################################

Pull_tweets  <- lapply(TopicSearch, function(i) {
                  Tweets <- search_tweets(q = i,
                                  n = 500)
                  Tweets$Keyword <- i
                  Tweets = data.table(Tweets)
            })
        
Pull_tweets_list <- rbindlist(Pull_tweets)


## Save to csv
#save_as_csv(Pull_tweets_list, paste("Pull_tweets_list_", Date, " .csv", sep=""), prepend_ids = TRUE, na = "", fileEncoding = "UTF-8")

##########################################################################
### Show timeline
##########################################################################

## plot time series (if ggplot2 is installed)
TIMELINE = Pull_tweets_list  %>%
  
  #Filter   
   group_by(Keyword) %>%
   filter(is_retweet==F ) %>%
   
  #Plot
   ts_plot(size=1) +
    theme_bw()+
    theme(text = element_text(size=15),
          strip.text.x = element_text(size=13, face="bold"),
          strip.background = element_rect(colour="white", fill="#FFFFFF"),
          panel.border = element_rect(colour = "black", size=2),
          legend.title = element_text(face="bold"),
          legend.text = element_text(size=16))+
    labs(
          title = "Number of tweets per keyword",
          subtitle = "Tweets collected, parsed, and plotted using `rtweet`") +
    scale_color_paletteer_d(jcolors, rainbow) 

print(TIMELINE)
 
```

## What is all the talk about?

```{r results=TRUE, message=FALSE, warning=FALSE, echo=FALSE, fig.align="center", fig.width=8, fig.height=9, fig.cap="_**Figure**: number of tweets talking about European business associations"}
##########################################################################
######### Analyse words in last 7 days
##########################################################################

COUNTALL = Pull_tweets_list  %>%

  # Filter out all tweets with mention of Green Tweets
  #filter(grepl(paste(GreenTweet, collapse="|"), text)) %>% 
  
  group_by(Keyword) %>%
  unnest_tokens(word, text) %>%
  count(word, sort = TRUE) %>%

  # Clean up text
  anti_join(stop_words) %>%
  filter(!word %in% c("t.co", "report", "https", tolower(TopicSearch), "eu", "de", "amp", "role")) %>% 
  top_n(10) %>%
  
  ## reordering
  group_by(word) %>%
  mutate(word_sum=sum(n)) %>%
  ungroup()%>%
  mutate(word = reorder(word, word_sum)) %>%

   ggplot(aes(x = word, y = n)) +
  geom_col(aes( fill=Keyword), colour="black") +
  theme_bw() +
  scale_color_paletteer_d(jcolors, rainbow) +
  coord_flip() +
  scale_fill_paletteer_d(quickpalette, dreaming) +
  guides(fill=guide_legend("Keyword")) +
  theme(text = element_text(size=15),
        strip.text.x = element_text(size=12, face="bold"),
        strip.background = element_rect(colour="white", fill="#FFFFFF"),
        panel.border = element_rect(colour = "black", size=2),
        legend.title = element_text(face="bold"),
        legend.text = element_text(size=16))+
  labs(y = "Number of times used",
       x = "Unique words",
       title = paste("Count of unique words in tweets", sep=""),
       subtitle=paste("[", Date, " snapshot]", sep=""))

print(COUNTALL)

# Compile overview plot (using patvhwork devtools::install_github("thomasp85/patchwork"))

# OVERVIEW = TIMELINE + COUNTALL +  plot_layout(ncol =1, heights = c(1, 2))
                 
# print(OVERVIEW)

```

## Who are tweeting about these topics?


```{r results=TRUE, message=FALSE, warning=FALSE, echo=FALSE, fig.align="center", fig.width=8, fig.height=5, fig.cap="_**Figure**: number of tweets talking about European business associations"}

##########################################################################
#### Accounts tweeting on business associations' green tweets:
##########################################################################

COUNTGREEN = Pull_tweets_list  %>%
  #filter(grepl(paste(GreenTweet, collapse="|"), text)) %>%
  group_by(Keyword) %>%
  #unnest_tokens(word, text) %>%
  count(screen_name) %>%
  top_n(10) %>%
 ungroup() %>%
  
  # reordering
  group_by(screen_name) %>%
  mutate(screen_name_sum=sum(n)) %>%
  ungroup()%>%
  mutate(screen_name = reorder(screen_name, screen_name_sum)) %>%
  
  #Filter more
  filter(!n <= 1) %>%
  
  #plotting
  ggplot(aes(x = screen_name, y = n)) +
  geom_col(aes( fill=Keyword), colour="black") +
  theme_bw() +
  scale_color_paletteer_d(jcolors, rainbow) +
  coord_flip() +
  scale_fill_paletteer_d(quickpalette, dreaming) +
  guides(fill=guide_legend("Keyword")) +
  theme(text = element_text(size=15),
        strip.text.x = element_text(size=12, face="bold"),
        strip.background = element_rect(colour="white", fill="#FFFFFF"),
        panel.border = element_rect(colour = "black", size=2),
        legend.title = element_text(face="bold"),
        legend.text = element_text(size=16))+
  labs(y = "Number of times used",
       x = "Unique words",
       title = paste("Count of Tweets per Tweeting person [", Date, " snapshot]", sep=""),
       subtitle=paste("Filtered on:", paste(GreenTweet, collapse=",")))  

```

## Who are the top influencers they are following?

```{r results=TRUE, message=FALSE, warning=FALSE, echo=FALSE, fig.align="center", fig.width=8, fig.height=8, fig.cap="_**Figure**: number of tweets talking about European business associationstop influencers being followed"}


############################################################################
### Which accounts are they following & are most influential (followers count)

## get user IDs of accounts followed by CNN
Friends_EUBusAss <- get_friends(TopicSearch)

## lookup data on those accounts 
Friends_EUBusAss_twitter_handles <- lookup_users(Friends_EUBusAss$user_id) 

Creepy_table <- merge(Friends_EUBusAss,Friends_EUBusAss_twitter_handles, by=c("user_id") )

Creepy_table_toptaggers <- Creepy_table %>%
                  group_by(user) %>%
                  top_n(followers_count, n=50) 
  


Creepy_table_overview=Creepy_table_toptaggers[order( Creepy_table_toptaggers$user, Creepy_table_toptaggers$followers_count), c('user', 'screen_name', 'followers_count')]

FOLLOWERS <- Creepy_table_overview  %>%
  top_n(20) %>%
  
    # reordering
  group_by(screen_name) %>%
  mutate(screen_name_sum=sum(followers_count)) %>%
  ungroup()%>%
  mutate(screen_name = reorder(screen_name, screen_name_sum)) %>%
  
  ggplot(aes(x = screen_name, y = followers_count)) +
  geom_col(aes( fill=user), colour="black") +
  theme_bw() +
  scale_color_paletteer_d(jcolors, rainbow) +
  #facet_wrap(~user, scales = "free") +
  coord_flip() +
  scale_fill_paletteer_d(quickpalette, dreaming) +
  guides(fill=guide_legend("Year")) +
  theme(text = element_text(size=15),
        strip.text.x = element_text(size=12, face="bold"),
        strip.background = element_rect(colour="white", fill="#FFFFFF"),
        panel.border = element_rect(colour = "black", size=2),
        legend.title = element_text(face="bold"),
        legend.text = element_text(size=16))+
  labs(x = "Twitter accounts",
       y = "Followers count",
       title = "Most influential accounts followed")  

print(FOLLOWERS)

```

## Who are the top influencers following them?

```{r results=TRUE, message=FALSE, warning=FALSE, echo=FALSE, fig.align="center", fig.width=10, fig.height=11, fig.cap="_**Figure**: Following top influences"}
########################################################################
######### Following these associations


## It is limited in how many calls you an make??
Following_me_EUBusAss  <- lapply(TopicSearch, function(i) {
                              List <- get_followers(i, n=500)
                              List=data.table(List)
                              List$user <- i
                              List=data.table(List)
                                
                                                  })

Following_me_EUBusAss_list <- do_call_rbind(Following_me_EUBusAss)

## lookup data on those accounts 
Following_me_EUBusAss_list_twitter_handles <- lookup_users(Following_me_EUBusAss_list$user_id) 

Following_me_Creepy_table <- merge(Following_me_EUBusAss_list,Following_me_EUBusAss_list_twitter_handles, by=c("user_id") )

Following_me_Creepy_table_toptaggers <- Following_me_Creepy_table %>%
  group_by(user) %>%
  top_n(followers_count, n=50) 

Following_me_Creepy_table_overview=Following_me_Creepy_table_toptaggers[order( Following_me_Creepy_table_toptaggers$user, 
                                                                               Following_me_Creepy_table_toptaggers$followers_count), 
                                                                        c('user', 'screen_name', 'followers_count')]

FOLLOWERS <- Following_me_Creepy_table  %>%
              select(user, screen_name, followers_count) %>%
              #filter(followers_count < 10000 & followers_count > 50)  %>%
              group_by(user) %>%
              top_n(20) %>%
  
  # reordering
  group_by(screen_name) %>%
  mutate(screen_name_sum=sum(followers_count)) %>%
  ungroup()%>%
  mutate(screen_name = reorder(screen_name, screen_name_sum)) %>%
  
  
              ggplot(aes(x = screen_name, y = followers_count)) +
              geom_col(aes( fill=user), colour="black") +
              theme_bw() +
              scale_color_paletteer_d(jcolors, rainbow) +
              #facet_wrap(~user, scales = "free") +
              
              #scale_y_log10()+
              coord_flip() +
  scale_fill_paletteer_d(quickpalette, dreaming) +
  guides(fill=guide_legend("Year")) +
  theme(text = element_text(size=15),
        strip.text.x = element_text(size=12, face="bold"),
        strip.background = element_rect(colour="white", fill="#FFFFFF"),
        panel.border = element_rect(colour = "black", size=2),
        legend.title = element_text(face="bold"),
        legend.text = element_text(size=16))+
              labs(x = "Twitter accounts",
                   y = "Followers count",
                   title = "Most influential accounts that are followers")  

print(FOLLOWERS)


```

## Word Network: Are there sector-specific or shared topics?

```{r results=TRUE, message=FALSE, warning=FALSE, echo=FALSE, fig.align="center", fig.width=12, fig.height=9, fig.cap="**Figure**: Are certain topics sector-specific or are they also shared?"}


################################
### PLUCKING TIMELINES
## Pull Twitter timelines

Pluck_tweets  <- lapply(TopicSearch, function(i) {
  List <- get_timeline(i, n = 2000, parse=T)
  List$Keyword <- i
  List = data.table(List)
})

Pluck_tweets_list <- do_call_rbind(Pluck_tweets)
Pluck_tweets_list$plain <- plain_tweets(Pluck_tweets_list$text)


################################

######### Network analysis

NETWORK_TML = Pluck_tweets_list %>%
  
  mutate(Year=as.numeric(substr(created_at, 1,4))) %>%
  
  # find paired words
  group_by(Keyword) %>%
  select(Keyword, plain, followers_count, Year) %>%
  unnest_tokens(paired_words, plain, token="ngrams", n=2) %>%
  
  #count
  count(paired_words, sort=TRUE) %>%
  
  # split
  separate(paired_words, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% c(stop_words$word, stop_words_nl$word, stop_words_du$word,"https", "t.co")) %>%
  filter(!word2 %in% c(stop_words$word, "https", "t.co")) 

# new count: dont know why you would do this???
#count(word1, word2, sort = TRUE) 

# plot
NETWORKPLOT2 = NETWORK_TML %>%  
  group_by(Keyword) %>%
  filter(n >= 10) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), colour="lightblue") +
  geom_node_point(colour="blue", size = 3) +
  geom_node_text(aes(label = name), vjust = 1.8, size = 3) +
  labs(title = "Word Network of Timeline Tweets",
       subtitle = "Text mining twitter data ",
       x = "", y = "", caption = "\nSource: Data collected from Twitter's REST API via rtweet") + 
  theme_bw() 
  #ggsave(paste("Word_Network_Timeline_tweets_Industry_", Date,".png", sep=""), width = 10, height = 7, units = c("in"), dpi=300) 


print(NETWORKPLOT2)





```


